{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 有記憶的神經網路\n",
    "要記得前面發生的事，將前一次的輸出當成這一次的輸入\n",
    "\n",
    "我們終於要介紹三大神經網路的最後一個, 也就是 RNN。RNN 有不少的變型, 例如 LSTM 和 GRU 等等, 不過我們都通稱叫 RNN。RNN 是一種「有記憶」的神經網路, 非常適合時間序列啦, 或是不定長度的輸入資料。\n",
    "\n",
    "有三個閥門 像神經元\n",
    "\n",
    "我們來看看怎麼樣用 RNN 做電影評論的「情意分析」, 也就是知道一則評論究竟是「正評」還是「負評」。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 初始準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND=tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 讀入 IMDB 電影數據庫\n",
    "今天我們要評入 IMDB 電影數據庫影評的部份。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 82s 5us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 輸入資料部份\n",
    "我們來看一下輸入部份長什麼樣子?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要注意這裡我們限制只選「最常用」1 萬字, 也就是超過這範圍的就當不存在。這是文字分析常會做的事。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意這其實是一個 list 而不是 array, 原因是每筆資料 (每段影評) 長度自然是不一樣的! 我們檢查一下前 10 筆的長度就可以知道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 47,\n",
       " 1678,\n",
       " 72,\n",
       " 19,\n",
       " 61,\n",
       " 205,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 146,\n",
       " 6,\n",
       " 655,\n",
       " 2212,\n",
       " 1021,\n",
       " 336,\n",
       " 7,\n",
       " 107,\n",
       " 146,\n",
       " 179,\n",
       " 2,\n",
       " 11,\n",
       " 61,\n",
       " 965,\n",
       " 5,\n",
       " 1624,\n",
       " 113,\n",
       " 131,\n",
       " 13,\n",
       " 28,\n",
       " 1460,\n",
       " 39,\n",
       " 61,\n",
       " 501,\n",
       " 15,\n",
       " 13,\n",
       " 361,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 1341,\n",
       " 39,\n",
       " 63,\n",
       " 13,\n",
       " 244,\n",
       " 1364,\n",
       " 9026,\n",
       " 54,\n",
       " 134,\n",
       " 2,\n",
       " 5,\n",
       " 68,\n",
       " 2166,\n",
       " 1497,\n",
       " 68,\n",
       " 456,\n",
       " 19,\n",
       " 72,\n",
       " 36,\n",
       " 339,\n",
       " 72,\n",
       " 8,\n",
       " 4392,\n",
       " 61,\n",
       " 113,\n",
       " 5,\n",
       " 61,\n",
       " 649,\n",
       " 19,\n",
       " 61,\n",
       " 223,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 123,\n",
       " 1025,\n",
       " 111,\n",
       " 409,\n",
       " 2,\n",
       " 83,\n",
       " 4,\n",
       " 501,\n",
       " 7,\n",
       " 4,\n",
       " 6176,\n",
       " 5,\n",
       " 2672,\n",
       " 687,\n",
       " 15,\n",
       " 242,\n",
       " 2164,\n",
       " 68,\n",
       " 6644,\n",
       " 111,\n",
       " 7,\n",
       " 178,\n",
       " 2712,\n",
       " 88,\n",
       " 45,\n",
       " 99,\n",
       " 629,\n",
       " 8,\n",
       " 140,\n",
       " 145,\n",
       " 5,\n",
       " 81,\n",
       " 17,\n",
       " 2676,\n",
       " 3088,\n",
       " 560,\n",
       " 4,\n",
       " 3808,\n",
       " 5,\n",
       " 921,\n",
       " 5607,\n",
       " 7,\n",
       " 4,\n",
       " 882,\n",
       " 11,\n",
       " 4,\n",
       " 2267,\n",
       " 5,\n",
       " 955,\n",
       " 479,\n",
       " 7,\n",
       " 263,\n",
       " 1548,\n",
       " 4,\n",
       " 123,\n",
       " 1015,\n",
       " 6,\n",
       " 176,\n",
       " 7,\n",
       " 1109,\n",
       " 18,\n",
       " 33,\n",
       " 222,\n",
       " 397,\n",
       " 14,\n",
       " 1773,\n",
       " 645,\n",
       " 14,\n",
       " 7847,\n",
       " 9,\n",
       " 1350,\n",
       " 5,\n",
       " 878,\n",
       " 21,\n",
       " 290,\n",
       " 12,\n",
       " 38,\n",
       " 76,\n",
       " 9029,\n",
       " 7,\n",
       " 6644,\n",
       " 2615,\n",
       " 5,\n",
       " 701,\n",
       " 2615,\n",
       " 186,\n",
       " 8,\n",
       " 2754,\n",
       " 4,\n",
       " 5029,\n",
       " 1341,\n",
       " 400,\n",
       " 45,\n",
       " 5015,\n",
       " 15,\n",
       " 4,\n",
       " 6176,\n",
       " 43,\n",
       " 31,\n",
       " 251,\n",
       " 645,\n",
       " 8,\n",
       " 1258,\n",
       " 56,\n",
       " 42,\n",
       " 845,\n",
       " 18,\n",
       " 253,\n",
       " 42,\n",
       " 1742,\n",
       " 42,\n",
       " 532,\n",
       " 602,\n",
       " 5,\n",
       " 150,\n",
       " 36,\n",
       " 191,\n",
       " 570,\n",
       " 24,\n",
       " 38,\n",
       " 2,\n",
       " 26,\n",
       " 44,\n",
       " 880,\n",
       " 1460,\n",
       " 13,\n",
       " 70,\n",
       " 2198,\n",
       " 8,\n",
       " 4,\n",
       " 275,\n",
       " 687,\n",
       " 5,\n",
       " 2,\n",
       " 11,\n",
       " 2851,\n",
       " 456,\n",
       " 50,\n",
       " 26,\n",
       " 1141,\n",
       " 1326,\n",
       " 5,\n",
       " 1767,\n",
       " 5630,\n",
       " 111,\n",
       " 2,\n",
       " 28,\n",
       " 3117,\n",
       " 4309,\n",
       " 2569,\n",
       " 49,\n",
       " 362,\n",
       " 331,\n",
       " 8261,\n",
       " 913,\n",
       " 8,\n",
       " 4313,\n",
       " 8,\n",
       " 148,\n",
       " 37,\n",
       " 104,\n",
       " 15,\n",
       " 6644,\n",
       " 9,\n",
       " 35,\n",
       " 120,\n",
       " 2097,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 13,\n",
       " 62,\n",
       " 43,\n",
       " 135,\n",
       " 15,\n",
       " 275,\n",
       " 84,\n",
       " 8261,\n",
       " 5897,\n",
       " 261,\n",
       " 49,\n",
       " 362,\n",
       " 2885,\n",
       " 4313,\n",
       " 73,\n",
       " 409,\n",
       " 40,\n",
       " 2,\n",
       " 11,\n",
       " 4,\n",
       " 123,\n",
       " 8307,\n",
       " 11,\n",
       " 6,\n",
       " 7961,\n",
       " 23,\n",
       " 4,\n",
       " 1873,\n",
       " 5,\n",
       " 28,\n",
       " 68,\n",
       " 456,\n",
       " 1437,\n",
       " 1194,\n",
       " 34,\n",
       " 4,\n",
       " 1494,\n",
       " 10,\n",
       " 10,\n",
       " 18,\n",
       " 463,\n",
       " 236,\n",
       " 2,\n",
       " 2,\n",
       " 301,\n",
       " 15,\n",
       " 184,\n",
       " 185,\n",
       " 2,\n",
       " 4846,\n",
       " 2,\n",
       " 39,\n",
       " 349,\n",
       " 59,\n",
       " 2,\n",
       " 18,\n",
       " 2208,\n",
       " 18,\n",
       " 6,\n",
       " 5451,\n",
       " 291,\n",
       " 154,\n",
       " 3608,\n",
       " 5,\n",
       " 1625,\n",
       " 349,\n",
       " 2569,\n",
       " 41,\n",
       " 481,\n",
       " 1081,\n",
       " 8,\n",
       " 259,\n",
       " 4,\n",
       " 201,\n",
       " 9,\n",
       " 1061,\n",
       " 19,\n",
       " 1618,\n",
       " 15,\n",
       " 75,\n",
       " 70,\n",
       " 361,\n",
       " 8,\n",
       " 391,\n",
       " 263,\n",
       " 205,\n",
       " 4484,\n",
       " 5,\n",
       " 97,\n",
       " 2,\n",
       " 8,\n",
       " 263,\n",
       " 456,\n",
       " 400,\n",
       " 45,\n",
       " 148,\n",
       " 7,\n",
       " 178,\n",
       " 19,\n",
       " 4139,\n",
       " 1341,\n",
       " 37,\n",
       " 2712,\n",
       " 4,\n",
       " 6309,\n",
       " 17,\n",
       " 36,\n",
       " 135,\n",
       " 60,\n",
       " 6,\n",
       " 2230,\n",
       " 106,\n",
       " 9,\n",
       " 208,\n",
       " 1453,\n",
       " 6,\n",
       " 251,\n",
       " 21,\n",
       " 6,\n",
       " 550,\n",
       " 106,\n",
       " 70,\n",
       " 140,\n",
       " 2,\n",
       " 18,\n",
       " 179,\n",
       " 6,\n",
       " 137,\n",
       " 366,\n",
       " 45,\n",
       " 93,\n",
       " 129,\n",
       " 113,\n",
       " 4309,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 4,\n",
       " 1180,\n",
       " 1294,\n",
       " 25,\n",
       " 18,\n",
       " 231,\n",
       " 4,\n",
       " 123,\n",
       " 18,\n",
       " 7847,\n",
       " 83,\n",
       " 4,\n",
       " 501,\n",
       " 18,\n",
       " 4,\n",
       " 794,\n",
       " 1969,\n",
       " 82,\n",
       " 4,\n",
       " 2876,\n",
       " 4,\n",
       " 2718,\n",
       " 5,\n",
       " 4,\n",
       " 756,\n",
       " 228,\n",
       " 26,\n",
       " 530,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 1294,\n",
       " 25,\n",
       " 18,\n",
       " 129,\n",
       " 3158,\n",
       " 8,\n",
       " 1497,\n",
       " 726,\n",
       " 42,\n",
       " 24,\n",
       " 25,\n",
       " 28,\n",
       " 1678,\n",
       " 624,\n",
       " 25,\n",
       " 28,\n",
       " 1678,\n",
       " 72]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 輸出的部分\n",
    "\n",
    "輸出方面應該很容易想像, 我們來看看前 10 筆。結果自然就是 0 (負評) 或 1 (正評)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料處理\n",
    "### 送入神經網路的輸入處理\n",
    "雖然 RNN 是可以處理不同長度的輸入, 在寫程式時我們還是要\n",
    "\n",
    "* 設輸入文字長度的上限\n",
    "* 把每段文字都弄成一樣長, 太短的後面補上 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=150)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 150)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 設計我們的情意分析 RNN¶\n",
    "這裡我們選用 LSTM, 基本上用哪種 RNN 寫法都是差不多的!\n",
    "\n",
    "\n",
    "### 決定神經網路架構\n",
    "先將 10000 維的文字壓到 N 維\n",
    "然後用 K 個 LSTM 神經元做隱藏層\n",
    "最後一個 output, 直接用 sigmoid 送出\n",
    "\n",
    "### 建構我們的神經網路\n",
    "文字我們用 1-hot 表示是很標準的方式, 不過要注意的是, 因為我們指定要 1 萬個字, 所以每個字是用 1 萬維的向量表示! 這一來很浪費記憶空間, 二來字和字間基本上是沒有關係的。我們可以用某種「合理」的方式, 把字壓到比較小的維度, 這些向量又代表某些意思 (比如說兩個字代表的向量角度小表相關程度大) 等等。\n",
    "\n",
    "這聽來很複雜的事叫 \"word embedding\", 而事實上 Keras 會幫我們做。我們只需告訴 Keras 原來最大的數字是多少 (10000), 還有我們打算壓到幾維 (N)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 10000維 -> 壓到3維(自訂)\n",
    "* RNN:10\n",
    "* Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3 # 文字要壓到 N 維\n",
    "K = 10 # LSTM 有 K 個神經元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM, GRU 都是做RNN的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step01. 建置RNN神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(10000, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM 層, 我們做 K 個 LSTM Cells。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid 將數值壓到0-1之間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 組裝\n",
    "\n",
    "這次我們用 binary_crossentropy 做我們的 loss function, 另外用一個很潮的 Adam 學習法。\n",
    "\n",
    "Adam會自動調整learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 3)           30000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10)                560       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 30,571\n",
      "Trainable params: 30,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN: \n",
    "\n",
    "    4x(13x10+10(bias))\n",
    "\n",
    "有三個閥門 加本身"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*(13*10+10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step02. 訓練\n",
    "\n",
    "我們用的 embedding 中, 會被 batch_size 影響輸入。輸入的 shape 會是\n",
    "\n",
    "    (batch_size, 每筆上限)\n",
    "\n",
    "也就是 (32,100) 輸出是 (32,100,128), 其中 128 是我們決定要壓成幾維的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 48s 2ms/step - loss: 0.4881 - accuracy: 0.7520\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.2711 - accuracy: 0.8948\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.2136 - accuracy: 0.9195\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.1779 - accuracy: 0.9378\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.1560 - accuracy: 0.9460\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 42s 2ms/step - loss: 0.1321 - accuracy: 0.9555\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.1183 - accuracy: 0.9602\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.1031 - accuracy: 0.9673\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.0907 - accuracy: 0.9713\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0804 - accuracy: 0.9761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1393dbef0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "         batch_size=32,\n",
    "         epochs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step03. 檢視結果\n",
    "#### 分數\n",
    "我們照例來看看測試資料的分數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 10s 402us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "測試資料的 loss = 0.5919722938346863\n",
      "測試資正確率 = 0.8369600176811218\n"
     ]
    }
   ],
   "source": [
    "print(f'測試資料的 loss = {score[0]}')\n",
    "print(f'測試資正確率 = {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5919722938346863, 0.8369600176811218]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 儲存結果\n",
    "這裡有 8 成我們可以正確分辨, 看來還不差, 照例我們把結果存檔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "open('imdb_model_arch.json','w').write(model_json)\n",
    "model.save_weights('imdb_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          1, 4700, 2755, 5696,    2,   39,  658,   18,  701, 3383, 1007,\n",
       "         63,   26,  684,  532, 2472,   23,    4, 1320,   59,  461,    2,\n",
       "         19,    2, 1224, 5939, 1138, 2319,  323,    5, 6062, 4946,   21,\n",
       "       4595, 1002,    4,  552,    7,   20,   15,  100,   64,   28,   77,\n",
       "          2,   11,    4, 3141,  290,    6,  168,   18,   94,  701, 2063,\n",
       "        114,    5,   18,  402,  354,   34,  705,  381,   10,   10, 1332,\n",
       "       3473,  127,   24,  193,  245,   99,   76,   39,  360,    7,  509,\n",
       "         63,  287,   35,  221, 5228,    7,  113], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[333]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talk to transformer\n",
    "\n",
    "## GPT-2\n",
    "## Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
